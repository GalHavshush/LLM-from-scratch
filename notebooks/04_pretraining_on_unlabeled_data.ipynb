{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8745407c",
   "metadata": {},
   "source": [
    "# 4. Pretraining On Unlabeled Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35facadc",
   "metadata": {},
   "source": [
    "## 4.1 Evaluating Generative Text Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42323e59",
   "metadata": {},
   "source": [
    "### 4.1.1 GPT-124M Configuration Setup and Model Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4e4fb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_modules import GPTModel\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 256,  # Context length\n",
    "    \"emb_dim\": 768,          # Embedding dimension\n",
    "    \"n_heads\": 12,           # Number of attention heads\n",
    "    \"n_layers\": 12,          # Number of layers\n",
    "    \"drop_rate\": 0.1,        # Dropout rate\n",
    "    \"qkv_bias\": False        # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ced3198e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffe10fa",
   "metadata": {},
   "source": [
    "### 4.1.2 Text-to-Tokens, Generation, and Decoding Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ad81020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you rentingetic wasnم refres RexMeCHicular stren\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "from gpt_modules import generate_text_simple\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'}) # Encode text into token IDs (allow GPT special tokens if present)\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # Add batch dimension → shape becomes (1, seq_len)\n",
    "    return encoded_tensor\n",
    "\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # Remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist()) # Convert token IDs back to readable text\n",
    "\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# Generate new tokens autoregressively\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "# Decode generated token IDs back into text\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88da246d",
   "metadata": {},
   "source": [
    "### 4.1.3 Calculating The Text Generation Loss: Cross-Entropy And Perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6718e10",
   "metadata": {},
   "source": [
    "Example Input and Target Token Batches for Next-Token Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e6dcbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([\n",
    "    [16833, 3626, 6100],   # \"every effort moves\"\n",
    "    [40,    1107, 588]     # \"I really like\"\n",
    "])\n",
    "\n",
    "targets = torch.tensor([\n",
    "    [3626, 6100, 345],     # \"effort moves you\"\n",
    "    [1107, 588, 11311]     # \"really like chocolate\"\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb8e773",
   "metadata": {},
   "source": [
    "Computing Token Probabilities from Model Logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa0ba999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():                # Disable gradients for inference\n",
    "    logits = model(inputs)           # Forward pass → raw logits (batch, seq_len, vocab_size)\n",
    "\n",
    "probas = torch.softmax(logits, dim=-1)  # Convert logits to probabilities over vocabulary\n",
    "print(probas.shape)                      # Expected: (batch_size, seq_len, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ddc3c9",
   "metadata": {},
   "source": [
    "Selecting Most Probable Token IDs (Greedy Prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c16621df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[[16657],\n",
      "         [  339],\n",
      "         [42826]],\n",
      "\n",
      "        [[49906],\n",
      "         [29669],\n",
      "         [41751]]])\n"
     ]
    }
   ],
   "source": [
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)  \n",
    "# Select highest-probability token at each position (greedy decoding)\n",
    "\n",
    "print(\"Token IDs:\\n\", token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914c0554",
   "metadata": {},
   "source": [
    "Comparing Target Tokens with Model Predictions (Decoded Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b21a93d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets batch 1:  effort moves you\n",
      "Outputs batch 1:  Armed heNetflix\n"
     ]
    }
   ],
   "source": [
    "# Decode target tokens for batch 1\n",
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "\n",
    "# Decode predicted token IDs for batch 1\n",
    "# Flatten removes extra dimension from argmax output\n",
    "print(f\"Outputs batch 1: \"\n",
    "      f\"{token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1563cdc6",
   "metadata": {},
   "source": [
    "Extracting Model Probabilities for Target Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6bfeeca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: tensor([7.4541e-05, 3.1061e-05, 1.1563e-05])\n",
      "Text 2: tensor([1.0337e-05, 5.6776e-05, 4.7559e-06])\n"
     ]
    }
   ],
   "source": [
    "text_idx = 0\n",
    "# Get probabilities assigned to the correct target tokens for batch 1\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 1:\", target_probas_1)\n",
    "\n",
    "text_idx = 1\n",
    "# Same extraction for batch 2\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 2:\", target_probas_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6bba26",
   "metadata": {},
   "source": [
    "Computing Log Probabilities of Target Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c1b63a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -9.5042, -10.3796, -11.3677, -11.4798,  -9.7764, -12.2561])\n"
     ]
    }
   ],
   "source": [
    "# Combine probabilities from both batches and convert to log-probabilities\n",
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
    "\n",
    "print(log_probas)  # Log probabilities used in cross-entropy / likelihood calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7be1b0f",
   "metadata": {},
   "source": [
    "Computing Average Log Probability of Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00299cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-10.7940)\n"
     ]
    }
   ],
   "source": [
    "# Compute average log probability across all target tokens\n",
    "avg_log_probas = torch.mean(log_probas)\n",
    "\n",
    "print(avg_log_probas)  # Higher (less negative) means better predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30dcd64a",
   "metadata": {},
   "source": [
    "Computing Negative Average Log Probability (Loss Value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f58508c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "# Convert average log probability into negative log-likelihood (loss)\n",
    "neg_avg_log_probas = avg_log_probas * -1\n",
    "\n",
    "print(neg_avg_log_probas)  # Equivalent to cross-entropy style loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5ca89f",
   "metadata": {},
   "source": [
    "Inspecting Logits and Target Tensor Shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a7c628f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 3, 50257])\n",
      "Targets shape: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "print(\"Logits shape:\", logits.shape)    # Expected: (batch_size, seq_len, vocab_size)\n",
    "print(\"Targets shape:\", targets.shape)  # Expected: (batch_size, seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5b7abf",
   "metadata": {},
   "source": [
    "Flattening Logits and Targets for Loss Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9c434ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened logits: torch.Size([6, 50257])\n",
      "Flattened targets: torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "# Merge batch and sequence dimensions to match loss function expectations\n",
    "logits_flat = logits.flatten(0, 1)   # Shape: (batch_size * seq_len, vocab_size)\n",
    "targets_flat = targets.flatten()     # Shape: (batch_size * seq_len)\n",
    "\n",
    "print(\"Flattened logits:\", logits_flat.shape)\n",
    "print(\"Flattened targets:\", targets_flat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5943f0f5",
   "metadata": {},
   "source": [
    "Computing Cross-Entropy Loss for Next-Token Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf77b225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "# Compute cross-entropy loss between predicted logits and true token IDs\n",
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "\n",
    "print(loss)  # Standard language modeling loss (negative log-likelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f44cf2",
   "metadata": {},
   "source": [
    "### 4.1.4 Calculating the training and validation set losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf69758",
   "metadata": {},
   "source": [
    "Loading Text Dataset and Counting Characters & Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "33b0533e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 20479\n",
      "Tokens: 5145\n"
     ]
    }
   ],
   "source": [
    "file_path = \"../dataset/the_verdict.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    text_data = file.read()\n",
    "    \n",
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b1d218",
   "metadata": {},
   "source": [
    "Splitting Dataset into Training and Validation Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "230906a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.90                              # Use 90% of data for training\n",
    "split_idx = int(train_ratio * len(text_data))   # Compute split index\n",
    "\n",
    "train_data = text_data[:split_idx]              # Training portion of text\n",
    "val_data = text_data[split_idx:]                # Validation portion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cece679e",
   "metadata": {},
   "source": [
    "Creating Training and Validation DataLoaders for GPT Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f4f12f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_modules import create_dataloader_v1\n",
    "torch.manual_seed(123)  # Ensure reproducible dataset shuffling\n",
    "\n",
    "# Training DataLoader\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],  # Sequence length\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],      # Non-overlapping chunks\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "# Validation DataLoader\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e63f265",
   "metadata": {},
   "source": [
    "Inspecting Batch Shapes from Training and Validation DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c9e8eda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)   # Input tokens and target tokens per batch\n",
    "\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)   # Same check for validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a209b6f3",
   "metadata": {},
   "source": [
    "Batch and Dataset Loss Computation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c6d815e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    \"\"\"\n",
    "    Compute cross-entropy loss for a single batch.\n",
    "\n",
    "    Moves data to device, runs forward pass,\n",
    "    flattens logits/targets for token-level loss.\n",
    "    \"\"\"\n",
    "\n",
    "    input_batch = input_batch.to(device)     # Move inputs to CPU/GPU\n",
    "    target_batch = target_batch.to(device)   # Move targets to same device\n",
    "    logits = model(input_batch)              # Forward pass → (batch, seq_len, vocab_size)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten()) # Flatten batch + sequence dims for cross-entropy loss\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    \"\"\"\n",
    "    Compute average loss over a DataLoader.\n",
    "\n",
    "    Optionally limit evaluation to a subset of batches\n",
    "    (useful for faster validation during training).\n",
    "    \"\"\"\n",
    "\n",
    "    total_loss = 0.\n",
    "\n",
    "    # Handle empty loader edge case\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "\n",
    "    # Use all batches unless specified otherwise\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "\n",
    "    # Iterate over batches\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(\n",
    "                input_batch, target_batch, model, device\n",
    "            )\n",
    "            total_loss += loss.item()   # Accumulate scalar loss\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # Return average loss per batch\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "19088658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10.987582842508951\n",
      "Validation loss: 10.98110580444336\n"
     ]
    }
   ],
   "source": [
    "# Prefer CUDA GPU → then Apple MPS → otherwise CPU\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "model.to(device)  # Move model to selected device\n",
    "\n",
    "with torch.no_grad():  # Disable gradients for evaluation\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232232f9",
   "metadata": {},
   "source": [
    "## 4.2 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ca91dacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc69833",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
